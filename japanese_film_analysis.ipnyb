import requests
import pandas as pd
import time
import re
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
API_KEY = "TMDB_API_KEY_REMOVED"  # <--- PASTE YOUR KEY HERE
BASE_URL = "https://api.themoviedb.org/3"

# --- HELPER FUNCTIONS ---

def clean_title_for_slug(title):
    """
    Converts a title like "Kiki's Delivery Service" to "kikis-delivery-service"
    to guess the Letterboxd URL.
    """
    # Lowercase, remove special chars (keep spaces and hyphens), strip whitespace
    slug = re.sub(r'[^a-zA-Z0-9\s-]', '', title.lower()).strip()
    # Replace spaces with hyphens
    slug = re.sub(r'\s+', '-', slug)
    return slug

def get_tmdb_movies(year, limit=20):
    """
    Fetches the top 'limit' Japanese movies for a specific year from TMDb.
    """
    movies_data = []
    page = 1
    
    print(f"ðŸŽ¬ Fetching movies from TMDb for {year}...")
    
    while len(movies_data) < limit:
        url = f"{BASE_URL}/discover/movie"
        params = {
            "api_key": API_KEY,
            "with_original_language": "ja",
            "primary_release_year": year,
            "sort_by": "popularity.desc",
            "page": page
        }
        res = requests.get(url, params=params).json()
        
        if not res.get('results'):
            break # No more results
            
        for m in res['results']:
            if len(movies_data) >= limit:
                break
            movies_data.append({
                "tmdb_id": m['id'],
                "title": m['title'],
                "original_title": m['original_title'],
                "release_date": m['release_date'],
                "tmdb_rating": m['vote_average'],
                "popularity": m['popularity']
            })
        page += 1
        
    return movies_data

def get_extended_details(tmdb_id):
    """
    Fetches the Director and Genres for a specific movie ID.
    """
    url = f"{BASE_URL}/movie/{tmdb_id}"
    params = {"api_key": API_KEY, "append_to_response": "credits"}
    res = requests.get(url, params=params).json()
    
    director = "Unknown"
    if 'credits' in res:
        for crew in res['credits']['crew']:
            if crew['job'] == 'Director':
                director = crew['name']
                break
                
    genres = [g['name'] for g in res.get('genres', [])]
    return director, ", ".join(genres)

def get_letterboxd_rating(title, year, original_title=None):
    """
    Improved scraper: Tries English title, then Original title, 
    and checks multiple HTML locations for the rating.
    """
    # Try both titles to increase hit rate
    titles_to_try = [title]
    if original_title and original_title != title:
        titles_to_try.append(original_title)
    
    # Standard headers to look like a real Mac user on Chrome
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    for t in titles_to_try:
        slug = clean_title_for_slug(t)
        url = f"https://letterboxd.com/film/{slug}/"
        
        try:
            response = requests.get(url, headers=headers, timeout=5)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Method A: Check the 'twitter:data2' meta tag (Standard)
                meta_tag = soup.find("meta", attrs={"name": "twitter:data2"})
                if meta_tag:
                    # Example: "3.92 out of 5" -> extract "3.92"
                    return meta_tag["content"].split(" ")[0]
                
                # Method B: Check the structured JSON-LD data (Very Reliable)
                import json
                script_tag = soup.find("script", type="application/ld+json")
                if script_tag:
                    data = json.loads(script_tag.string)
                    if 'aggregateRating' in data:
                        return data['aggregateRating']['ratingValue']
            
            # If we get here, it means this specific title/slug didn't work
            time.sleep(0.5) 
            
        except Exception as e:
            continue
            
    return None # Truly not found
# --- MAIN EXECUTION ---

# 1. Ask User for Year
target_year = input("Enter the year you want to analyze (e.g., 2001): ")
num_movies = int(input("How many movies do you want to fetch? (Rec: 20-50 for testing): "))

# 2. Get the Base List
movies = get_tmdb_movies(target_year, limit=num_movies)

# 3. Loop and Enrich (The ETL Step)
final_data = []

print(f"\nðŸš€ Starting enrichment for {len(movies)} movies. This may take a moment...\n")

for i, movie in enumerate(movies):
    print(f"[{i+1}/{len(movies)}] Processing: {movie['title']}...", end="\r")
    
    # Get Director (API Call)
    director, genres = get_extended_details(movie['tmdb_id'])
    
    # Get Letterboxd Rating (Scrape)
    lb_rating = get_letterboxd_rating(movie['title'], target_year)
    
    # Add to our final list
    row = movie.copy()
    row['director'] = director
    row['genres'] = genres
    row['lb_rating'] = lb_rating
    final_data.append(row)
    
    # Polite sleep to avoid blocking
    time.sleep(1) 

# 4. Create DataFrame and Save
df = pd.DataFrame(final_data)

# Reorder columns for SQL friendliness
df = df[['tmdb_id', 'title', 'year', 'director', 'genres', 'lb_rating', 'tmdb_rating', 'popularity', 'release_date']] if 'year' in df.columns else df

csv_filename = f"japanese_films_{target_year}.csv"
df.to_csv(csv_filename, index=False)

print(f"\n\nâœ… Done! Saved {len(df)} movies to '{csv_filename}'.")
print(df.head())